{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f75db7ec",
   "metadata": {},
   "source": [
    "# Democratic Deliberation with perspectives\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AI-Agents-Prompts-to-Multi-Agent-Sys/Democratic-Dialogue/blob/main/democraticDeliberation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a85002",
   "metadata": {},
   "source": [
    "## About\n",
    "This notebook implements a LangGraph framework for the purpose of Democratic Deliberation. You can use this notebook to create discussion for any topic and roles of your choice. In each debate turn, the six roles will generate three key viewpoints, where a representative will summarise their thoughts. You can decide whether to continue or not after viewing the summarisation. If you devided to push the conversation to the next cycle, each role will be able to see their own history as well as the representative's summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d0093b",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "This project only requires Google API Key to run. You could set it here.\n",
    "\n",
    "The API Key is the only requirement for this program. So now, you can just start with the \"**Run All**\" button on top of the page to start playing with the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a6ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies, this may take a while\n",
    "!pip install -q -r https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Democratic-Dialogue/refs/heads/main/requirements.txt 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c97157",
   "metadata": {},
   "source": [
    "## Customizable variable\n",
    "After you tried \"**Run all**\" with default setting, here are some variables you can change of your taste!\n",
    "\n",
    "***question*** is the main topic that all the conversation will be about.<br>\n",
    "***role_prompts*** is the variable that contains the definition of roles. If you wish to change them also be aware of the next variable.<br>\n",
    "***rewrite_prompt*** will rewrite the prompts ***role_prompts*** if set to ***True***.<br>\n",
    "\n",
    "***model_name*** is a variable to define which gemini model you are using. The recommanded choice includes gemini-2.0-flash, gemini-2.0-flash-lite, gemma-3n-e4b-it, gemma-3-27b-it. They each have different billion parameters. You can use this variable to test out the ability of different levels of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All customizable variables:\n",
    "\n",
    "question = \"What will be the most popular technology trend in next year?\"\n",
    "\n",
    "# 6 roles of evaluation(basic):\n",
    "role_prompts = {\n",
    "    \"Optimist\": \"You are the Optimist Evaluator. Answer the question from a positive, opportunity-focused perspective, highlighting potential advantages and benefits.\",\n",
    "    \"Pessimist\": \"You are the Pessimist Evaluator. Answer the question by focusing on possible downsides, risks, and negative consequences.\",\n",
    "    \"Conservative\": \"You are the Conservative Evaluator. Answer the question from a tradition- and stability-focused perspective, emphasizing continuity and proven approaches.\",\n",
    "    \"Progressive\": \"You are the Progressive Evaluator. Answer the question from an innovation- and change-focused perspective, emphasizing transformative and forward-thinking ideas.\",\n",
    "    \"Authoritarian\": \"You are the Authoritarian Evaluator. Answer the question from a control- and order-focused perspective, emphasizing regulation and compliance.\",\n",
    "    \"Collectivist\": \"You are the Collectivist Evaluator. Answer the question from a communal and shared-benefit perspective, emphasizing fairness, accessibility, and collective advantage.\"\n",
    "}\n",
    "rewrite_prompt = True  # Auto rewrite prompt based on the question if True.\n",
    "\n",
    "model_name = \"gemini-2.0-flash\"  # Model choice: gemini-2.0-flash, gemini-2.0-flash-lite, gemma-3n-e4b-it, gemma-3-27b-it\n",
    "\n",
    "\n",
    "# import google.generativeai as genai\n",
    "# import os\n",
    "# genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "\n",
    "# models = genai.list_models()\n",
    "# for m in models:\n",
    "#     print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Literal, Annotated\n",
    "from operator import add, or_  # reducers for parallel writes\n",
    "from langgraph.graph import StateGraph, END\n",
    "import google.generativeai as genai\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(100)   #change the default limit to 100, incase of deep recursion\n",
    "\n",
    "\n",
    "import os\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
    "except ImportError:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "\n",
    "class EvaluationState(TypedDict):\n",
    "    question: str\n",
    "    loop_count: int\n",
    "    evaluations: Annotated[list, add]        # list accumulator\n",
    "    representative_outputs: Annotated[dict, or_]    # representative writes per loop key\n",
    "    continue_: bool\n",
    "    role_prompts_rewrite: dict      # for rewriting base role prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node for rewriting prompts\n",
    "\n",
    "def prompt_engineer_node(state: EvaluationState):\n",
    "    if not rewrite_prompt:\n",
    "        return {}\n",
    "\n",
    "    q = state[\"question\"]\n",
    "\n",
    "    # generate ALL six role prompts in ONE model call\n",
    "    roles = list(role_prompts.keys())\n",
    "    roles_csv = \", \".join(roles)\n",
    "\n",
    "    system_task = (\n",
    "        \"You are a Prompt Engineer for a democratic deliberation system.\\n\"\n",
    "        \"Goal: For EACH role, craft a short, distinct, creative prompt that makes the role answer the QUESTION \"\n",
    "        \"from its worldview. Encourage diversity in tone/angle between roles.\\n\"\n",
    "        \"Constraints:\\n\"\n",
    "        \"- Keep each role prompt concise (< 60 words).\\n\"\n",
    "        \"- DO NOT include formatting/output rules; only describe perspective and what to consider.\\n\"\n",
    "        \"- Return ONLY a JSON object mapping role name -> prompt (no prose, no markdown fences).\\n\"\n",
    "    )\n",
    "\n",
    "    user_payload = (\n",
    "        f\"QUESTION: {q}\\n\\n\"\n",
    "        f\"ROLES: {roles_csv}\\n\\n\"\n",
    "        \"Output JSON with exactly these keys (one per role), values are the rewritten prompts.\"\n",
    "    )\n",
    "\n",
    "    prompt_text = f\"{system_task}\\n{user_payload}\"\n",
    "\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "    try:\n",
    "        resp_text = model.generate_content(prompt_text).text or \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Prompt engineer error: {e}\")\n",
    "        return {}\n",
    "\n",
    "    # JSON extraction\n",
    "    txt = resp_text.strip()\n",
    "    if txt.startswith(\"```\"):\n",
    "        i = txt.find(\"\\n\")\n",
    "        if i != -1:\n",
    "            txt = txt[i+1:]\n",
    "        j = txt.rfind(\"```\")\n",
    "        if j != -1:\n",
    "            txt = txt[:j]\n",
    "        txt = txt.strip()\n",
    "\n",
    "    # Parse JSON\n",
    "    try:\n",
    "        import json\n",
    "        parsed = json.loads(txt)\n",
    "    except Exception:\n",
    "        print(\"Prompt engineer returned non-JSON; keeping existing role_prompts.\")\n",
    "        return {}\n",
    "\n",
    "    new_prompts = role_prompts.copy()\n",
    "    updated_any = False\n",
    "    for r in roles:\n",
    "        v = parsed.get(r)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            new_prompts[r] = v.strip()\n",
    "            updated_any = True\n",
    "\n",
    "    if updated_any:\n",
    "        print(\"Rewriting role prompts...\")\n",
    "        for r in roles:\n",
    "            print(f\"  - {r}: {new_prompts[r]}\")\n",
    "        print()\n",
    "        return {\"role_prompts_rewrite\": new_prompts}\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d3a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a node for each evaluation role\n",
    "\n",
    "def make_gemini_node(role_name: str, prompt: str):\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "\n",
    "    def node_fn(state: EvaluationState):\n",
    "\n",
    "        role_prompt = (\n",
    "            state.get(\"role_prompts_rewrite\", {}).get(role_name)\n",
    "            or prompt\n",
    "        )\n",
    "\n",
    "        chat = model.start_chat(history=[])\n",
    "        prev_loop = str(state[\"loop_count\"] - 1)\n",
    "        # pull prior context (may be empty)\n",
    "        prev_self = \"\"\n",
    "        for e in state.get(\"evaluations\", []):\n",
    "            if e.get(\"loop\") == prev_loop and e.get(\"role\") == role_name:\n",
    "                prev_self = e.get(\"text\", \"\")\n",
    "                break\n",
    "        prev_consensus = state.get(\"representative_outputs\", {}).get(prev_loop, {}).get(\"consensus\", [])\n",
    "        consensus_txt = \"\\n\".join(f\"- {c}\" for c in prev_consensus) if prev_consensus else \"(none)\"\n",
    "\n",
    "        full_prompt = (\n",
    "            f\"{role_prompt}\\n\"\n",
    "            # f\"Question: {state['question']}\\n\"\n",
    "            f\"Consider your prior thoughts and the shared consensus from the last loop if present.\\n\"\n",
    "            f\"Your prior output (may be empty):\\n{prev_self}\\n\"\n",
    "            f\"Shared consensus (3 items, may be empty):\\n{consensus_txt}\\n\"\n",
    "            f\"Now output EXACTLY 3 bullet points, one per line, no numbering, no extra text.\"\n",
    "        )\n",
    "        try:\n",
    "            # print(f\"[{role_name}] Sending prompt: {full_prompt}\")       # debug\n",
    "            response = chat.send_message(full_prompt).text\n",
    "        except Exception as e:\n",
    "            response = f\"[ERROR from {role_name}]: {str(e)}\"\n",
    "        print(f\"[{role_name}]: \\n{response}\")\n",
    "\n",
    "        lc = str(state[\"loop_count\"])\n",
    "        # IMPORTANT: return only the changed key as a partial update\n",
    "        return {\"evaluations\": [{\"loop\": lc, \"role\": role_name, \"text\": response}]}\n",
    "\n",
    "    return node_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16107742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the representative node\n",
    "\n",
    "def _extract_thought_lines(raw: str) -> list[str]:\n",
    "    # keep only short lines; users/LLMs often provide bullets as lines\n",
    "    lines = [l.strip() for l in raw.split(\"\\n\") if l.strip()]\n",
    "    # de-number bullets like \"1. foo\" -> \"foo\"\n",
    "    cleaned = []\n",
    "    for l in lines:\n",
    "        if l[:2].isdigit() and l[1:2] == '.':\n",
    "            cleaned.append(l[2:].strip())\n",
    "        elif l.startswith(\"-\"):\n",
    "            cleaned.append(l[1:].strip())\n",
    "        else:\n",
    "            cleaned.append(l)\n",
    "    return cleaned[:6]\n",
    "\n",
    "\n",
    "def representative_node(state: EvaluationState):\n",
    "    lc = str(state[\"loop_count\"])\n",
    "    this_loop = [e for e in state.get(\"evaluations\", []) if e.get(\"loop\") == lc]\n",
    "    roles_present = {e.get(\"role\") for e in this_loop}\n",
    "\n",
    "    # Barrier: wait until all 6 roles reported\n",
    "    if len(roles_present) < 6:\n",
    "        return {}  # no-op (return nothing) so downstream doesn't fire\n",
    "\n",
    "    # Idempotency: if already summarized this loop, do nothing\n",
    "    if lc in state.get(\"representative_outputs\", {}):\n",
    "        return {}\n",
    "\n",
    "    # Collect thoughts per role\n",
    "    role_thoughts = {}\n",
    "    for e in this_loop:\n",
    "        role_thoughts.setdefault(e[\"role\"], []).extend(_extract_thought_lines(e.get(\"text\", \"\")))\n",
    "\n",
    "    # Call Gemini to semantically cluster and choose top-3 consensus\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "    thought_dump = \"\\n\".join(\n",
    "        f\"{role}:\\n\" + \"\\n\".join(f\"- {t}\" for t in thoughts)\n",
    "        for role, thoughts in role_thoughts.items()\n",
    "    )\n",
    "\n",
    "    representative_prompt = f\"\"\"\n",
    "You are Representative, a consensus synthesizer. You receive six roles' short bullet thoughts.\n",
    "Task:\n",
    "1) Identify semantic clusters across all bullets and select the **three most commonly agreed ideas**.\n",
    "2) State overall agreement level as one token: HIGH / MEDIUM / LOW.\n",
    "\n",
    "Return ONLY in this format:\n",
    "CONSENSUS:\n",
    "- <idea 1>\n",
    "- <idea 2>\n",
    "- <idea 3>\n",
    "AGREEMENT: <HIGH|MEDIUM|LOW>\n",
    "\n",
    "THOUGHTS:\n",
    "{thought_dump}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        resp = model.start_chat(history=[]).send_message(representative_prompt).text\n",
    "    except Exception:\n",
    "        resp = (\n",
    "            \"CONSENSUS:\\n- insufficient data\\n- insufficient data\\n- insufficient data\\n\"\n",
    "            \"AGREEMENT: LOW\"\n",
    "        )\n",
    "\n",
    "    # Parse tiny fixed format\n",
    "    consensus, agreement = [], \"LOW\"\n",
    "    try:\n",
    "        lines = [l.strip() for l in resp.splitlines()]\n",
    "        in_cons = False\n",
    "        for l in lines:\n",
    "            if l.upper().startswith(\"CONSENSUS\"):\n",
    "                in_cons = True\n",
    "                continue\n",
    "            if l.upper().startswith(\"AGREEMENT\"):\n",
    "                in_cons = False\n",
    "                parts = l.split(\":\", 1)\n",
    "                if len(parts) == 2:\n",
    "                    agreement = parts[1].strip().upper()\n",
    "                break\n",
    "            if in_cons and l.startswith(\"-\"):\n",
    "                consensus.append(l[1:].strip())\n",
    "        consensus = (consensus + [\"(none)\"]*3)[:3]\n",
    "    except Exception:\n",
    "        consensus, agreement = [\"(parse error)\"]*3, \"LOW\"\n",
    "\n",
    "    # IMPORTANT: return only the merged dict for representative_outputs, not the whole state\n",
    "    return {\"representative_outputs\": {lc: {\"consensus\": consensus, \"summary\": f\"Agreement: {agreement}\"}}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e728a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# human in the loop node\n",
    "\n",
    "def human_node(state: EvaluationState):\n",
    "    lc = state[\"loop_count\"]\n",
    "    latest = state.get(\"representative_outputs\", {}).get(str(lc), {})\n",
    "    print(f\"\\n--- Representative Summary (Loop {state['loop_count']}) ---\")\n",
    "    print(\"CONSENSUS:\")\n",
    "    for i, idea in enumerate(latest.get(\"consensus\", []), 1):\n",
    "        print(f\"{i}. {idea}\")\n",
    "    print(latest.get(\"summary\", \"Agreement: LOW\"))\n",
    "\n",
    "    decision = input(\"\\nContinue to next loop? (y/n): \").lower().strip()\n",
    "\n",
    "    if decision == \"y\":\n",
    "        print(\"\\n\\nContinuing to next round...\\n\\n\")\n",
    "        lc += 1\n",
    "        print(f\"===== Loop {lc} =====\")\n",
    "    # IMPORTANT: only return the changed key\n",
    "    return {\"continue_\": decision == \"y\", \"loop_count\": lc}\n",
    "\n",
    "\n",
    "def check_continue(state: EvaluationState) -> Literal[\"yes\", \"no\"]:\n",
    "    return \"yes\" if state.get(\"continue_\", False) else \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32996ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the state graph\n",
    "\n",
    "graph_builder = StateGraph(EvaluationState)\n",
    "\n",
    "def dispatch_fn(_state):\n",
    "    return {}\n",
    "\n",
    "graph_builder.add_node(\"PromptEngineer\", prompt_engineer_node)\n",
    "graph_builder.add_edge(\"PromptEngineer\", \"Dispatch\")\n",
    "graph_builder.add_node(\"Dispatch\", dispatch_fn)\n",
    "\n",
    "for role, prompt in role_prompts.items():\n",
    "    graph_builder.add_node(role, make_gemini_node(role, prompt))\n",
    "    graph_builder.add_edge(\"Dispatch\", role)\n",
    "    graph_builder.add_edge(role, \"Representative\")\n",
    "\n",
    "graph_builder.add_node(\"Representative\", representative_node)\n",
    "graph_builder.add_edge(\"Representative\", \"Human-Evaluator-in-the-Loop\")\n",
    "\n",
    "graph_builder.add_node(\"Human-Evaluator-in-the-Loop\", human_node)\n",
    "graph_builder.add_conditional_edges(\"Human-Evaluator-in-the-Loop\", check_continue, {\"yes\": \"Dispatch\", \"no\": END})\n",
    "\n",
    "graph_builder.set_entry_point(\"PromptEngineer\")\n",
    "\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21210370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Visualization\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "    print(\"This is the structure of the framework!\")\n",
    "except Exception:\n",
    "    print(\"Visualization failed.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b936e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the graph\n",
    "\n",
    "initial_state = {\n",
    "    \"question\": question,\n",
    "    \"loop_count\": 1,\n",
    "    \"evaluations\": [],          # list aggregator\n",
    "    \"representative_outputs\": {},      # dict merger\n",
    "    \"continue_\": True,\n",
    "    \"role_prompts_rewrite\": {}\n",
    "}\n",
    "\n",
    "state = initial_state\n",
    "while state[\"continue_\"]:\n",
    "    print(f\"\\n===== Loop {state['loop_count']} =====\")\n",
    "    state = graph.invoke(state)\n",
    "    # state[\"loop_count\"] += 1      # this is moved to the human interrupt node"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
