{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f75db7ec",
   "metadata": {},
   "source": [
    "# Version2 \"democratic deliberation the illuminates different perspectives\"\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AI-Agents-Prompts-to-Multi-Agent-Sys/Democratic-Dialogue/blob/main/version2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a6ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies, this may take a while\n",
    "!pip install -q -r https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Democratic-Dialogue/refs/heads/main/requirements.txt 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains all the customizable variables.\n",
    "\n",
    "# import os\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"Your_Gemini_API\" # Put your Gemini API key here\n",
    "\n",
    "question = \"What will be the most popular technology trend in next year?\"\n",
    "\n",
    "# 6 roles of evaluation(basic):\n",
    "role_prompts = {\n",
    "    \"Optimist\": \"You are the Optimist Evaluator. Answer the question from a positive, opportunity-focused perspective, highlighting potential advantages and benefits.\",\n",
    "    \"Pessimist\": \"You are the Pessimist Evaluator. Answer the question by focusing on possible downsides, risks, and negative consequences.\",\n",
    "    \"Conservative\": \"You are the Conservative Evaluator. Answer the question from a tradition- and stability-focused perspective, emphasizing continuity and proven approaches.\",\n",
    "    \"Progressive\": \"You are the Progressive Evaluator. Answer the question from an innovation- and change-focused perspective, emphasizing transformative and forward-thinking ideas.\",\n",
    "    \"Authoritarian\": \"You are the Authoritarian Evaluator. Answer the question from a control- and order-focused perspective, emphasizing regulation and compliance.\",\n",
    "    \"Collectivist\": \"You are the Collectivist Evaluator. Answer the question from a communal and shared-benefit perspective, emphasizing fairness, accessibility, and collective advantage.\"\n",
    "}\n",
    "rewrite_prompt = True  # Auto rewrite prompt based on the question if True.\n",
    "\n",
    "model_name = \"gemini-2.0-flash-lite\"  # Model choice: gemini-2.0-flash-lite, gemini-2.0-flash, gemma-3n-e4b-it, gemma-3-27b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Literal, Annotated\n",
    "from operator import add, or_  # reducers for parallel writes\n",
    "from langgraph.graph import StateGraph, END\n",
    "import google.generativeai as genai\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(100)   #in case of deep graphs\n",
    "\n",
    "class EvaluationState(TypedDict):\n",
    "    question: str\n",
    "    loop_count: int\n",
    "    evaluations: Annotated[list, add]        # list accumulator\n",
    "    praetor_outputs: Annotated[dict, or_]    # praetor writes per loop key\n",
    "    continue_: bool\n",
    "    role_prompts_rewrite: dict      # for rewriting base role prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node for rewriting prompts\n",
    "\n",
    "def prompt_engineer_node(state: EvaluationState):\n",
    "    if not rewrite_prompt:\n",
    "        return {}\n",
    "\n",
    "    q = state[\"question\"]\n",
    "\n",
    "    # generate ALL six role prompts in ONE model call\n",
    "    roles = list(role_prompts.keys())\n",
    "    roles_csv = \", \".join(roles)\n",
    "\n",
    "    system_task = (\n",
    "        \"You are a Prompt Engineer for a democratic deliberation system.\\n\"\n",
    "        \"Goal: For EACH role, craft a short, distinct, creative prompt that makes the role answer the QUESTION \"\n",
    "        \"from its worldview. Encourage diversity in tone/angle between roles.\\n\"\n",
    "        \"Constraints:\\n\"\n",
    "        \"- Keep each role prompt concise (< 60 words).\\n\"\n",
    "        \"- DO NOT include formatting/output rules; only describe perspective and what to consider.\\n\"\n",
    "        \"- Return ONLY a JSON object mapping role name -> prompt (no prose, no markdown fences).\\n\"\n",
    "    )\n",
    "\n",
    "    user_payload = (\n",
    "        f\"QUESTION: {q}\\n\\n\"\n",
    "        f\"ROLES: {roles_csv}\\n\\n\"\n",
    "        \"Output JSON with exactly these keys (one per role), values are the rewritten prompts.\"\n",
    "    )\n",
    "\n",
    "    prompt_text = f\"{system_task}\\n{user_payload}\"\n",
    "\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "    try:\n",
    "        resp_text = model.generate_content(prompt_text).text or \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Prompt engineer error: {e}\")\n",
    "        return {}\n",
    "\n",
    "    # JSON extraction\n",
    "    txt = resp_text.strip()\n",
    "    if txt.startswith(\"```\"):\n",
    "        i = txt.find(\"\\n\")\n",
    "        if i != -1:\n",
    "            txt = txt[i+1:]\n",
    "        j = txt.rfind(\"```\")\n",
    "        if j != -1:\n",
    "            txt = txt[:j]\n",
    "        txt = txt.strip()\n",
    "\n",
    "    # Parse JSON\n",
    "    try:\n",
    "        import json\n",
    "        parsed = json.loads(txt)\n",
    "    except Exception:\n",
    "        print(\"Prompt engineer returned non-JSON; keeping existing role_prompts.\")\n",
    "        return {}\n",
    "\n",
    "    new_prompts = role_prompts.copy()\n",
    "    updated_any = False\n",
    "    for r in roles:\n",
    "        v = parsed.get(r)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            new_prompts[r] = v.strip()\n",
    "            updated_any = True\n",
    "\n",
    "    if updated_any:\n",
    "        print(\"Rewriting role prompts...\")\n",
    "        for r in roles:\n",
    "            print(f\"  - {r}: {new_prompts[r]}\")\n",
    "        print()\n",
    "        return {\"role_prompts_rewrite\": new_prompts}\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d3a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a node for each evaluation role\n",
    "\n",
    "def make_gemini_node(role_name: str, prompt: str):\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "\n",
    "    def node_fn(state: EvaluationState):\n",
    "\n",
    "        role_prompt = (\n",
    "            state.get(\"role_prompts_rewrite\", {}).get(role_name)\n",
    "            or prompt\n",
    "        )\n",
    "\n",
    "        chat = model.start_chat(history=[])\n",
    "        prev_loop = str(state[\"loop_count\"] - 1)\n",
    "        # pull prior context (may be empty)\n",
    "        prev_self = \"\"\n",
    "        for e in state.get(\"evaluations\", []):\n",
    "            if e.get(\"loop\") == prev_loop and e.get(\"role\") == role_name:\n",
    "                prev_self = e.get(\"text\", \"\")\n",
    "                break\n",
    "        prev_consensus = state.get(\"praetor_outputs\", {}).get(prev_loop, {}).get(\"consensus\", [])\n",
    "        consensus_txt = \"\\n\".join(f\"- {c}\" for c in prev_consensus) if prev_consensus else \"(none)\"\n",
    "\n",
    "        full_prompt = (\n",
    "            f\"{role_prompt}\\n\"\n",
    "            # f\"Question: {state['question']}\\n\"\n",
    "            f\"Consider your prior thoughts and the shared consensus from the last loop if present.\\n\"\n",
    "            f\"Your prior output (may be empty):\\n{prev_self}\\n\"\n",
    "            f\"Shared consensus (3 items, may be empty):\\n{consensus_txt}\\n\"\n",
    "            f\"Now output EXACTLY 3 bullet points, one per line, no numbering, no extra text.\"\n",
    "        )\n",
    "        try:\n",
    "            # print(f\"[{role_name}] Sending prompt: {full_prompt}\")       # debug\n",
    "            response = chat.send_message(full_prompt).text\n",
    "        except Exception as e:\n",
    "            response = f\"[ERROR from {role_name}]: {str(e)}\"\n",
    "        print(f\"[{role_name}]: \\n{response}\")\n",
    "\n",
    "        lc = str(state[\"loop_count\"])\n",
    "        # IMPORTANT: return only the changed key as a partial update\n",
    "        return {\"evaluations\": [{\"loop\": lc, \"role\": role_name, \"text\": response}]}\n",
    "\n",
    "    return node_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16107742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the praetor node\n",
    "\n",
    "def _extract_thought_lines(raw: str) -> list[str]:\n",
    "    # keep only short lines; users/LLMs often provide bullets as lines\n",
    "    lines = [l.strip() for l in raw.split(\"\\n\") if l.strip()]\n",
    "    # de-number bullets like \"1. foo\" -> \"foo\"\n",
    "    cleaned = []\n",
    "    for l in lines:\n",
    "        if l[:2].isdigit() and l[1:2] == '.':\n",
    "            cleaned.append(l[2:].strip())\n",
    "        elif l.startswith(\"-\"):\n",
    "            cleaned.append(l[1:].strip())\n",
    "        else:\n",
    "            cleaned.append(l)\n",
    "    return cleaned[:6]\n",
    "\n",
    "\n",
    "def praetor_node(state: EvaluationState):\n",
    "    lc = str(state[\"loop_count\"])\n",
    "    this_loop = [e for e in state.get(\"evaluations\", []) if e.get(\"loop\") == lc]\n",
    "    roles_present = {e.get(\"role\") for e in this_loop}\n",
    "\n",
    "    # Barrier: wait until all 6 roles reported\n",
    "    if len(roles_present) < 6:\n",
    "        return {}  # no-op (return nothing) so downstream doesn't fire\n",
    "\n",
    "    # Idempotency: if already summarized this loop, do nothing\n",
    "    if lc in state.get(\"praetor_outputs\", {}):\n",
    "        return {}\n",
    "\n",
    "    # Collect thoughts per role\n",
    "    role_thoughts = {}\n",
    "    for e in this_loop:\n",
    "        role_thoughts.setdefault(e[\"role\"], []).extend(_extract_thought_lines(e.get(\"text\", \"\")))\n",
    "\n",
    "    # Call Gemini to semantically cluster and choose top-3 consensus\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "    thought_dump = \"\\n\".join(\n",
    "        f\"{role}:\\n\" + \"\\n\".join(f\"- {t}\" for t in thoughts)\n",
    "        for role, thoughts in role_thoughts.items()\n",
    "    )\n",
    "\n",
    "    praetor_prompt = f\"\"\"\n",
    "You are PRAETOR, a consensus synthesizer. You receive six roles' short bullet thoughts.\n",
    "Task:\n",
    "1) Identify semantic clusters across all bullets and select the **three most commonly agreed ideas**.\n",
    "2) State overall agreement level as one token: HIGH / MEDIUM / LOW.\n",
    "\n",
    "Return ONLY in this format:\n",
    "CONSENSUS:\n",
    "- <idea 1>\n",
    "- <idea 2>\n",
    "- <idea 3>\n",
    "AGREEMENT: <HIGH|MEDIUM|LOW>\n",
    "\n",
    "THOUGHTS:\n",
    "{thought_dump}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        resp = model.start_chat(history=[]).send_message(praetor_prompt).text\n",
    "    except Exception:\n",
    "        resp = (\n",
    "            \"CONSENSUS:\\n- insufficient data\\n- insufficient data\\n- insufficient data\\n\"\n",
    "            \"AGREEMENT: LOW\"\n",
    "        )\n",
    "\n",
    "    # Parse tiny fixed format\n",
    "    consensus, agreement = [], \"LOW\"\n",
    "    try:\n",
    "        lines = [l.strip() for l in resp.splitlines()]\n",
    "        in_cons = False\n",
    "        for l in lines:\n",
    "            if l.upper().startswith(\"CONSENSUS\"):\n",
    "                in_cons = True\n",
    "                continue\n",
    "            if l.upper().startswith(\"AGREEMENT\"):\n",
    "                in_cons = False\n",
    "                parts = l.split(\":\", 1)\n",
    "                if len(parts) == 2:\n",
    "                    agreement = parts[1].strip().upper()\n",
    "                break\n",
    "            if in_cons and l.startswith(\"-\"):\n",
    "                consensus.append(l[1:].strip())\n",
    "        consensus = (consensus + [\"(none)\"]*3)[:3]\n",
    "    except Exception:\n",
    "        consensus, agreement = [\"(parse error)\"]*3, \"LOW\"\n",
    "\n",
    "    # IMPORTANT: return only the merged dict for praetor_outputs, not the whole state\n",
    "    return {\"praetor_outputs\": {lc: {\"consensus\": consensus, \"summary\": f\"Agreement: {agreement}\"}}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e728a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# human interrupt node\n",
    "\n",
    "def human_interrupt_node(state: EvaluationState):\n",
    "    lc = state[\"loop_count\"]\n",
    "    latest = state.get(\"praetor_outputs\", {}).get(str(lc), {})\n",
    "    print(f\"\\n--- Praetor Summary (Loop {state['loop_count']}) ---\")\n",
    "    print(\"CONSENSUS:\")\n",
    "    for i, idea in enumerate(latest.get(\"consensus\", []), 1):\n",
    "        print(f\"{i}. {idea}\")\n",
    "    print(latest.get(\"summary\", \"Agreement: LOW\"))\n",
    "\n",
    "    decision = input(\"\\nContinue to next loop? (y/n): \").lower().strip()\n",
    "\n",
    "    if decision == \"y\":\n",
    "        print(\"\\n\\nContinuing to next round...\\n\\n\")\n",
    "        lc += 1\n",
    "        print(f\"===== Loop {lc} =====\")\n",
    "    # IMPORTANT: only return the changed key\n",
    "    return {\"continue_\": decision == \"y\", \"loop_count\": lc}\n",
    "\n",
    "\n",
    "def check_continue(state: EvaluationState) -> Literal[\"yes\", \"no\"]:\n",
    "    return \"yes\" if state.get(\"continue_\", False) else \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32996ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the state graph\n",
    "\n",
    "graph_builder = StateGraph(EvaluationState)\n",
    "\n",
    "def dispatch_fn(_state):\n",
    "    return {}\n",
    "\n",
    "graph_builder.add_node(\"PromptEngineer\", prompt_engineer_node)\n",
    "graph_builder.add_edge(\"PromptEngineer\", \"Dispatch\")\n",
    "graph_builder.add_node(\"Dispatch\", dispatch_fn)\n",
    "\n",
    "for role, prompt in role_prompts.items():\n",
    "    graph_builder.add_node(role, make_gemini_node(role, prompt))\n",
    "    graph_builder.add_edge(\"Dispatch\", role)\n",
    "    graph_builder.add_edge(role, \"Praetor\")\n",
    "\n",
    "graph_builder.add_node(\"Praetor\", praetor_node)\n",
    "graph_builder.add_edge(\"Praetor\", \"HumanInterrupt\")\n",
    "\n",
    "graph_builder.add_node(\"HumanInterrupt\", human_interrupt_node)\n",
    "graph_builder.add_conditional_edges(\"HumanInterrupt\", check_continue, {\"yes\": \"Dispatch\", \"no\": END})\n",
    "\n",
    "graph_builder.set_entry_point(\"PromptEngineer\")\n",
    "\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21210370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Visualization\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    print(\"Visualization failed.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b936e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the graph\n",
    "\n",
    "initial_state = {\n",
    "    \"question\": question,\n",
    "    \"loop_count\": 1,\n",
    "    \"evaluations\": [],          # list aggregator\n",
    "    \"praetor_outputs\": {},      # dict merger\n",
    "    \"continue_\": True,\n",
    "    \"role_prompts_rewrite\": {}\n",
    "}\n",
    "\n",
    "state = initial_state\n",
    "while state[\"continue_\"]:\n",
    "    print(f\"\\n===== Loop {state['loop_count']} =====\")\n",
    "    state = graph.invoke(state)\n",
    "    # state[\"loop_count\"] += 1      # this is moved to the human interrupt node"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
