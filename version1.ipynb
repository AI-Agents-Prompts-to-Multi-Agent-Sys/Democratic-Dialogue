{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f75db7ec",
   "metadata": {},
   "source": [
    "##### Version1<br>\"democratic deliberation the illuminates different perspectives\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d60a2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"<Your Gemini API>\" # Put your Gemini API key here\n",
    "\n",
    "question = \"When and how will AGI surpass human intelligence?\"\n",
    "\n",
    "# 6 roles of evaluation:\n",
    "role_prompts = {\n",
    "    \"Optimist\": \"You are the Optimist Evaluator. Provide 3 key benefits of the proposal.\",\n",
    "    \"Pessimist\": \"You are the Pessimist Evaluator. Provide 3 key risks of the proposal.\",\n",
    "    \"Conservative\": \"You are the Conservative Evaluator. Provide 3 key ideas\",\n",
    "    \"Progressive\": \"You are the Progressive Evaluator. Provide 3 key innovative aspects.\",\n",
    "    \"Authoritarian\": \"You are the Authoritarian Evaluator. Provide 3 key control mechanisms.\",\n",
    "    \"Collectivist\": \"You are the Collectivist Evaluator. Provide 3 key communal benefits.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b0956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AdamYE_1/Desktop/repos/Democratic-Dialogue/aiagent/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Literal, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "import google.generativeai as genai\n",
    "from operator import add, or_  # add for lists, or_ for dict merges\n",
    "\n",
    "\n",
    "class EvaluationState(TypedDict):\n",
    "    proposal: str\n",
    "    loop_count: int\n",
    "    evaluations: Annotated[list, add]        # list accumulator\n",
    "    praetor_outputs: Annotated[dict, or_]    # praetor writes per loop key\n",
    "    continue_: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d3a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a node for each evaluation role\n",
    "\n",
    "def make_gemini_node(role_name: str, prompt: str):\n",
    "    model = genai.GenerativeModel(\"gemini-2.0-flash-lite\")\n",
    "    def node_fn(state: EvaluationState):\n",
    "        chat = model.start_chat(history=[])\n",
    "        prev_loop = str(state[\"loop_count\"] - 1)\n",
    "        prev_self = state[\"evaluations\"].get(prev_loop, {}).get(role_name, \"\")\n",
    "        prev_consensus = state[\"praetor_outputs\"].get(prev_loop, {}).get(\"consensus\", [])\n",
    "        consensus_txt = \"\\n\".join(f\"- {c}\" for c in prev_consensus) if prev_consensus else \"(none)\"\n",
    "\n",
    "        full_prompt = (\n",
    "            f\"{prompt}\\n\\n\"\n",
    "            f\"Proposal:\\n{state['proposal']}\\n\\n\"\n",
    "            f\"Consider your prior thoughts and shared consensus from the last loop if present.\\n\"\n",
    "            f\"Your prior output (may be empty):\\n{prev_self}\\n\\n\"\n",
    "            f\"Shared consensus (3 items, may be empty):\\n{consensus_txt}\\n\\n\"\n",
    "            f\"Now output EXACTLY 3 bullet points, one per line, no numbering.\"\n",
    "        )\n",
    "        try:\n",
    "            response = chat.send_message(full_prompt).text\n",
    "        except Exception as e:\n",
    "            response = f\"[ERROR from {role_name}]: {str(e)}\"\n",
    "        print(f\"[{role_name}]: {response}\\n\")       # printing the response\n",
    "        lc = str(state[\"loop_count\"])\n",
    "        updated = dict(state)\n",
    "        if lc not in updated[\"evaluations\"]:\n",
    "            updated[\"evaluations\"][lc] = {}\n",
    "        updated[\"evaluations\"][lc][role_name] = response\n",
    "        return updated\n",
    "    return node_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16107742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the praetor node\n",
    "\n",
    "def _extract_thought_lines(raw: str) -> list[str]:\n",
    "    lines = [l.strip() for l in raw.split(\"\\n\") if l.strip()]\n",
    "    return lines[:6]\n",
    "\n",
    "def praetor_node(state: EvaluationState):\n",
    "    loop_key = str(state[\"loop_count\"])\n",
    "    roles_out = state[\"evaluations\"].get(loop_key, {})\n",
    "\n",
    "    # Barrier: wait until all 6 roles have produced output\n",
    "    if len(roles_out) < 6:\n",
    "        return state  # no-op, prevents downstream firing\n",
    "\n",
    "    # Idempotency: if already summarized this loop, do nothing\n",
    "    if loop_key in state[\"praetor_outputs\"]:\n",
    "        return state\n",
    "\n",
    "    # Gather thoughts per role (short, trimmed lines)\n",
    "    role_thoughts: dict[str, list[str]] = {}\n",
    "    for role, text in roles_out.items():\n",
    "        role_thoughts[role] = _extract_thought_lines(text)\n",
    "\n",
    "    # Build compact prompt for semantic clustering via Gemini\n",
    "    model = genai.GenerativeModel(\"gemini-2.0-flash-lite\")\n",
    "    thought_dump = \"\\n\".join(\n",
    "        f\"{role}:\\n\" + \"\\n\".join(f\"- {t}\" for t in thoughts)\n",
    "        for role, thoughts in role_thoughts.items()\n",
    "    )\n",
    "\n",
    "    praetor_prompt = f\"\"\"\n",
    "You are P R A E T O R, a consensus synthesizer. You receive six roles' short bullet thoughts.\n",
    "Task:\n",
    "1) Identify semantic clusters across all bullets and select the **three most commonly agreed ideas**.\n",
    "2) State overall agreement level as one token: HIGH / MEDIUM / LOW.\n",
    "3) Keep outputs concise and non-redundant.\n",
    "\n",
    "Return ONLY in this format:\n",
    "CONSENSUS:\n",
    "- <idea 1>\n",
    "- <idea 2>\n",
    "- <idea 3>\n",
    "AGREEMENT: <HIGH|MEDIUM|LOW>\n",
    "\n",
    "THOUGHTS:\n",
    "{thought_dump}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        resp = model.start_chat(history=[]).send_message(praetor_prompt).text\n",
    "    except Exception as e:\n",
    "        resp = \"\"\"CONSENSUS:\n",
    "- insufficient data\n",
    "- insufficient data\n",
    "- insufficient data\n",
    "AGREEMENT: LOW\"\"\"\n",
    "\n",
    "    # Parse the fixed format\n",
    "    consensus = []\n",
    "    agreement = \"LOW\"\n",
    "    try:\n",
    "        lines = [l.strip() for l in resp.splitlines()]\n",
    "        in_cons = False\n",
    "        for l in lines:\n",
    "            if l.upper().startswith(\"CONSENSUS\"):\n",
    "                in_cons = True\n",
    "                continue\n",
    "            if l.upper().startswith(\"AGREEMENT\"):\n",
    "                in_cons = False\n",
    "                parts = l.split(\":\", 1)\n",
    "                if len(parts) == 2:\n",
    "                    agreement = parts[1].strip().upper()\n",
    "                break\n",
    "            if in_cons and l.startswith(\"-\"):\n",
    "                consensus.append(l[1:].strip())\n",
    "        consensus = (consensus + [\"(none)\"]*3)[:3]\n",
    "    except Exception:\n",
    "        consensus = [\"(parse error)\"]*3\n",
    "        agreement = \"LOW\"\n",
    "\n",
    "    updated = dict(state)\n",
    "    updated[\"praetor_outputs\"][loop_key] = {\n",
    "        \"consensus\": consensus,\n",
    "        \"summary\": f\"Agreement: {agreement}\"\n",
    "    }\n",
    "    return updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80e728a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# human interrupt node\n",
    "\n",
    "def human_interrupt_node(state: EvaluationState):\n",
    "    print(f\"\\n--- Praetor Summary (Loop {state['loop_count']}) ---\")\n",
    "    latest = state[\"praetor_outputs\"][str(state[\"loop_count\"])]\n",
    "    print(latest[\"consensus\"])\n",
    "    print(\"\\n\" + latest[\"summary\"])\n",
    "\n",
    "    decision = input(\"\\nContinue to next loop? (y/n): \").lower().strip()\n",
    "    updated = dict(state)\n",
    "    updated[\"continue_\"] = decision == \"y\"\n",
    "    return updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5b6c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_continue(state: EvaluationState) -> Literal[\"yes\", \"no\"]:\n",
    "    return \"yes\" if state[\"continue_\"] else \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32996ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the state graph\n",
    "\n",
    "graph_builder = StateGraph(EvaluationState)\n",
    "\n",
    "def dispatch_fn(state): return state\n",
    "graph_builder.add_node(\"Dispatch\", dispatch_fn)\n",
    "\n",
    "for role, prompt in role_prompts.items():\n",
    "    graph_builder.add_node(role, make_gemini_node(role, prompt))\n",
    "    graph_builder.add_edge(\"Dispatch\", role)\n",
    "    graph_builder.add_edge(role, \"Praetor\")\n",
    "\n",
    "graph_builder.add_node(\"Praetor\", praetor_node)\n",
    "graph_builder.add_edge(\"Praetor\", \"HumanInterrupt\")\n",
    "\n",
    "graph_builder.add_node(\"HumanInterrupt\", human_interrupt_node)\n",
    "graph_builder.add_conditional_edges(\"HumanInterrupt\", check_continue, {\"yes\": \"Dispatch\", \"no\": END})\n",
    "\n",
    "graph_builder.set_entry_point(\"Dispatch\")\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21210370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Graph Visualization\n",
    "# from IPython.display import Image, display\n",
    "\n",
    "# try:\n",
    "#     display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "# except Exception:\n",
    "#     print(\"Visualization failed.\")\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b936e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Loop 1 =====\n",
      "[Optimist]: Here's the Optimist Evaluator's assessment of the proposal:\n",
      "\n",
      "*   Focused discussion sparks crucial debate about the future of AI.\n",
      "*   Encourages interdisciplinary collaboration to understand the complexities of AGI.\n",
      "*   Forces consideration of ethical implications and societal impact of advanced intelligence.\n",
      "\n",
      "\n",
      "[Authoritarian]: *   Establish a global AI oversight council composed of hand-picked experts and vetted government officials to dictate the pace and direction of AI development.\n",
      "*   Implement mandatory government-approved software licenses for all AI systems, restricting access and usage based on national security and societal benefit criteria.\n",
      "*   Monitor all AI research and development, immediately halting any projects deemed to pose an existential threat or challenge to established social order.\n",
      "\n",
      "[Conservative]: *   Precise timing of AGI surpassing human intelligence is impossible to predict, as progress hinges on breakthroughs in currently unpredictable areas like neuroscience or novel computing architectures.\n",
      "*   Defining and measuring \"human intelligence\" is complex, complicating any assessment of surpassing it; different tasks and metrics will yield varying results.\n",
      "*   The development of AGI will likely be a gradual, rather than a sudden, transition, with early capabilities possibly exceeding human performance in specific, narrowly defined domains.\n",
      "\n",
      "\n",
      "[Collectivist]: *   Centralized AGI development could ensure equitable access to advanced technologies, reducing disparities in resource distribution and opportunities across communities.\n",
      "*   A coordinated approach to AGI safety and alignment mitigates the existential risks associated with uncontrolled AI, safeguarding the collective future of humanity.\n",
      "*   Shared research and development efforts could accelerate the discovery of solutions to global challenges like climate change and disease, benefiting all members of society.\n",
      "\n",
      "\n",
      "\n",
      "[Progressive]: Here's my evaluation of the proposal, considering the lack of prior context:\n",
      "\n",
      "*   The question directly confronts a crucial and highly debated topic, forcing exploration of timelines and methodologies, which is innovative in its boldness.\n",
      "*   It necessitates delving into the metrics and definitions of \"intelligence,\" leading to potentially innovative discussions on the very nature of intelligence and consciousness.\n",
      "*   The proposal implicitly encourages examination of the societal impact of AGI, potentially sparking innovative thought about ethical considerations and future governance.\n",
      "\n",
      "\n",
      "[Pessimist]: *   The definition of \"surpassing human intelligence\" is subjective and potentially impossible to definitively measure, leading to continuous debate and preventing any meaningful consensus or action.\n",
      "*   Unforeseen consequences of AGI, given its inherently unpredictable nature, could rapidly lead to societal instability, resource depletion, or even existential threats that we are not prepared for.\n",
      "*   The concentration of AGI development and control in a few entities or nations creates a dangerous power imbalance, increasing the potential for misuse, intentional harm, or unintended escalation.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidUpdateError",
     "evalue": "At key 'proposal': Can receive only one value per step. Use an Annotated key to handle multiple values.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidUpdateError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m state[\u001b[33m\"\u001b[39m\u001b[33mcontinue_\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m===== Loop \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate[\u001b[33m'\u001b[39m\u001b[33mloop_count\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m =====\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     state = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     state[\u001b[33m\"\u001b[39m\u001b[33mloop_count\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/repos/Democratic-Dialogue/aiagent/lib/python3.13/site-packages/langgraph/pregel/__init__.py:2844\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, **kwargs)\u001b[39m\n\u001b[32m   2841\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   2842\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2844\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2845\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2846\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2847\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   2848\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   2849\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2850\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2851\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2852\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2853\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2854\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2855\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2856\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2857\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/repos/Democratic-Dialogue/aiagent/lib/python3.13/site-packages/langgraph/pregel/__init__.py:2544\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2534\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.tick(\n\u001b[32m   2535\u001b[39m             [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2536\u001b[39m             timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2539\u001b[39m         ):\n\u001b[32m   2540\u001b[39m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2541\u001b[39m             \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[32m   2542\u001b[39m                 stream_mode, print_mode, subgraphs, stream.get, queue.Empty\n\u001b[32m   2543\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m2544\u001b[39m         \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mafter_tick\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2546\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[32m   2547\u001b[39m     stream_mode, print_mode, subgraphs, stream.get, queue.Empty\n\u001b[32m   2548\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/repos/Democratic-Dialogue/aiagent/lib/python3.13/site-packages/langgraph/pregel/loop.py:526\u001b[39m, in \u001b[36mPregelLoop.after_tick\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    524\u001b[39m writes = [w \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tasks.values() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m t.writes]\n\u001b[32m    525\u001b[39m \u001b[38;5;66;03m# all tasks have finished\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m \u001b[38;5;28mself\u001b[39m.updated_channels = \u001b[43mapply_writes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheckpointer_get_next_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrigger_to_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[38;5;66;03m# produce values output\u001b[39;00m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.updated_channels.isdisjoint(\n\u001b[32m    535\u001b[39m     (\u001b[38;5;28mself\u001b[39m.output_keys,)\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.output_keys, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    537\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output_keys\n\u001b[32m    538\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/repos/Democratic-Dialogue/aiagent/lib/python3.13/site-packages/langgraph/pregel/algo.py:299\u001b[39m, in \u001b[36mapply_writes\u001b[39m\u001b[34m(checkpoint, channels, tasks, get_next_version, trigger_to_nodes)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chan, vals \u001b[38;5;129;01min\u001b[39;00m pending_writes_by_channel.items():\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chan \u001b[38;5;129;01min\u001b[39;00m channels:\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mchannels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchan\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m next_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    300\u001b[39m             checkpoint[\u001b[33m\"\u001b[39m\u001b[33mchannel_versions\u001b[39m\u001b[33m\"\u001b[39m][chan] = next_version\n\u001b[32m    301\u001b[39m             \u001b[38;5;66;03m# unavailable channels can't trigger tasks, so don't add them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/repos/Democratic-Dialogue/aiagent/lib/python3.13/site-packages/langgraph/channels/last_value.py:58\u001b[39m, in \u001b[36mLastValue.update\u001b[39m\u001b[34m(self, values)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values) != \u001b[32m1\u001b[39m:\n\u001b[32m     54\u001b[39m     msg = create_error_message(\n\u001b[32m     55\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAt key \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: Can receive only one value per step. Use an Annotated key to handle multiple values.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     56\u001b[39m         error_code=ErrorCode.INVALID_CONCURRENT_GRAPH_UPDATE,\n\u001b[32m     57\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(msg)\n\u001b[32m     60\u001b[39m \u001b[38;5;28mself\u001b[39m.value = values[-\u001b[32m1\u001b[39m]\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mInvalidUpdateError\u001b[39m: At key 'proposal': Can receive only one value per step. Use an Annotated key to handle multiple values.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE"
     ]
    }
   ],
   "source": [
    "# run the graph\n",
    "\n",
    "initial_state = {\n",
    "    \"proposal\": question,\n",
    "    \"loop_count\": 1,\n",
    "    \"evaluations\": {},\n",
    "    \"praetor_outputs\": {},\n",
    "    \"continue_\": True\n",
    "}\n",
    "\n",
    "state = initial_state\n",
    "while state[\"continue_\"]:\n",
    "    print(f\"\\n===== Loop {state['loop_count']} =====\")\n",
    "    state = graph.invoke(state)\n",
    "    state[\"loop_count\"] += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
